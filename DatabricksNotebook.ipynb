{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Installing the Iceberg ProtocolSwitchingFileIO JAR in Databricks\n",
    "\n",
    "This project provides tools for protocol switching file input/output in Apache Iceberg with Azure support.\n",
    "\n",
    "---\n",
    "\n",
    "## Building the JAR\n",
    "\n",
    "To package the project as a JAR file:\n",
    "\n",
    "1. Ensure you have [Maven](https://maven.apache.org/install.html) installed and configured on **LOCAL** your machine.\n",
    "2. Clone this repository and navigate into the `ProtocolSwitchingFileIO` directory.\n",
    "3. Run the following command to build the JAR:\n",
    "\n",
    "   ```bash\n",
    "   mvn clean package\n",
    "   ```\n",
    "\n",
    "   This will generate the JAR file in the `target` directory. For this project, the file will be named `ProtocolSwitchingFileIO-1.0.0.jar`.\n",
    "\n",
    "---\n",
    "\n",
    "## Uploading to Databricks\n",
    "\n",
    "1. **Upload the Library to Workspace Files**\n",
    "   - Open your Databricks workspace in a browser.\n",
    "   - In the left-hand menu, click **Workspace**.\n",
    "   - Navigate to the folder where you want to upload the JAR file.\n",
    "   - Click the kebab menu (three vertical dots) in the upper-right corner and select **Import**.\n",
    "   - In the **Import** dialog:\n",
    "     - Choose **File** as the import source.\n",
    "     - Drag and drop the `ProtocolSwitchingFileIO-1.0.0.jar` file or browse to select it.\n",
    "   - Click **Import**.\n",
    "\n",
    "2. **Install the Library on a Cluster**\n",
    "   - Go to the **Compute** tab in Databricks.\n",
    "   - Click the name of your cluster in the list.\n",
    "   - Click the **Libraries** tab and then **Install new**.\n",
    "   - In the **Install library** dialog:\n",
    "     - Select **Workspace** as the library source.\n",
    "     - Browse to the uploaded JAR file or specify its path, such as `/Workspace/Users/your-email@example.com/path-to-library/ProtocolSwitchingFileIO-1.0.0.jar`.\n",
    "   - Click **Install**.\n",
    "\n",
    "For additional instructions and alternative workflows, refer to [Databricks' official documentation](https://docs.databricks.com/en/libraries/workspace-files-libraries.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "667096d5-63c3-437d-bc80-24c5117245c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Polaris account and configuration\n",
    "account = \"aa0000.west-us-2.azure\"  # Replace with your Snowflake account\n",
    "principal_client_id = os.getenv(\"POLARIS_CLIENT_ID\", \"default_client_id\")  # Use environment variables\n",
    "principal_secret = os.getenv(\"POLARIS_SECRET\", \"default_secret\")  # Use environment variables\n",
    "catalog_name = \"protocalswitchingtest\"  # Replace with your catalog name\n",
    "role = \"PRINCIPAL_ROLE:data_engineer\"  # Replace with the role you are using\n",
    "\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName('Testing_ProtocolSwitchingFileIO') \\\n",
    "    .config('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.6.1,org.apache.iceberg:iceberg-azure-bundle:1.6.1') \\\n",
    "    .config('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions') \\\n",
    "    .config('spark.sql.defaultCatalog', 'opencatalog') \\\n",
    "    .config('spark.sql.catalog.opencatalog', 'org.apache.iceberg.spark.SparkCatalog') \\\n",
    "    .config('spark.sql.catalog.opencatalog.type', 'rest') \\\n",
    "    .config('spark.sql.catalog.opencatalog.header.X-Iceberg-Access-Delegation', 'vended-credentials') \\\n",
    "    .config('spark.sql.catalog.opencatalog.uri', f'https://{account}.snowflakecomputing.com/polaris/api/catalog') \\\n",
    "    .config('spark.sql.catalog.opencatalog.credential', f'{principal_client_id}:{principal_secret}') \\\n",
    "    .config('spark.sql.catalog.opencatalog.warehouse', catalog_name) \\\n",
    "    .config('spark.sql.catalog.opencatalog.scope', role) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Custom IO Configuration\n",
    "# Note: These configurations are specific to the ProtocolSwitchingFileIO JAR.\n",
    "# While these could be included in the Spark session builder config above, they are separated here\n",
    "# for clarity and to highlight their association with the ProtocolSwitchingFileIO functionality.\n",
    "spark.conf.set(\"spark.sql.catalog.opencatalog.io-impl\", \"org.apache.iceberg.tools.ProtocolSwitchingFileIO\")\n",
    "spark.conf.set(\"spark.sql.catalog.opencatalog.io-impl-delegate\", \"org.apache.iceberg.azure.adlsv2.ADLSFileIO\")\n",
    "spark.conf.set(\"spark.sql.catalog.opencatalog.protocol.mapping.wasb://\", \"abfs://\")\n",
    "spark.conf.set(\"spark.sql.catalog.opencatalog.protocol.mapping.wasbs://\", \"abfss://\")\n",
    "\n",
    "# Optional: Disable Vectorization for Compatibility\n",
    "# Note: This can impact performance. Only disable if required for compatibility.\n",
    "spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"false\")\n",
    "spark.conf.set(\"spark.sql.iceberg.vectorization.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "422c10f0-f0ab-467e-9afe-8c43eeccf276",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test to Verify JAR Import and Usage\n",
    "from py4j.java_gateway import java_import\n",
    "\n",
    "java_import(spark._jvm, \"org.apache.iceberg.tools.ProtocolSwitchingFileIO\")\n",
    "\n",
    "try:\n",
    "    # Attempt to initialize the class\n",
    "    wrapper = spark._jvm.org.apache.iceberg.tools.ProtocolSwitchingFileIO()\n",
    "    print(\"Class initialized successfully:\", wrapper)\n",
    "except Exception as e:\n",
    "    print(\"Error initializing ProtocolSwitchingFileIO class:\", e)\n",
    "\n",
    "try:\n",
    "    # Fetch the class version\n",
    "    version = spark._jvm.org.apache.iceberg.tools.ProtocolSwitchingFileIO.getVersion()\n",
    "    print(f\"ProtocolSwitchingFileIO version: {version}\")\n",
    "except Exception as e:\n",
    "    print(\"Error fetching ProtocolSwitchingFileIO version:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb0a0ba5-e98a-424f-84af-ac3967397c10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simple SQL Commands for Testing\n",
    "# Replace DATABASE.SCHEMA.TABLENAME with actual values\n",
    "try:\n",
    "    spark.sql(\"SHOW NAMESPACES\").show()\n",
    "    df = spark.sql(\"SELECT * FROM DATABASE.SCHEMA.TABLENAME LIMIT 10\")\n",
    "    df.show(10)  # Display the first 10 rows\n",
    "except Exception as e:\n",
    "    print(\"Error executing SQL commands:\", e)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ProtocolSwitchingFileIO Testing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
